{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"/Volumes/retail_catalog/retail_schema/retail_volume/Retail_Transactions_Dataset.csv\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual catalog/schema/volume names\n",
    "file_path = \"/Volumes/retail_catalog/retail_schema/retail_volume/Retail_Transactions_Dataset.csv\"\n",
    "\n",
    "# Load CSV into Spark DataFrame\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.limit(5))\n",
    "\n",
    "# Check schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# 1) Convert Date to timestamp\n",
    "df = df.withColumn(\"event_ts\", F.to_timestamp(F.col(\"Date\"), \"dd-MM-yyyy HH:mm\")) \\\n",
    "       .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Normalize text fields\n",
    "df = df.withColumn(\"city\", F.initcap(\"City\")) \\\n",
    "       .withColumn(\"store_type\", F.initcap(\"Store_Type\")) \\\n",
    "       .withColumn(\"payment_method\", F.initcap(\"Payment_Method\"))\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Parse Product list (convert single quotes to double quotes, then parse JSON)\n",
    "df = df.withColumn(\"products_json\", F.regexp_replace(F.col(\"Product\"), \"'\", '\"')) \\\n",
    "       .withColumn(\"products\", F.from_json(\"products_json\", T.ArrayType(T.StringType()))) \\\n",
    "       .drop(\"products_json\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38947751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Explode products into individual rows\n",
    "df_items = df.withColumn(\"product\", F.explode_outer(\"products\"))\n",
    "\n",
    "display(df_items.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Recompute using product count from the parsed array\n",
    "df_items = (df_items\n",
    "    .withColumn(\"computed_total_items\", F.size(\"products\"))\n",
    "    # Each product in the list is 1 unit\n",
    "    .withColumn(\"item_qty_est\", F.when(F.col(\"computed_total_items\") > 0, F.lit(1.0)).otherwise(F.lit(None)))\n",
    "    # Split the basket total_cost equally across the products\n",
    "    .withColumn(\"item_revenue_est\",\n",
    "                F.when(F.col(\"computed_total_items\") > 0,\n",
    "                       F.col(\"Total_Cost\") / F.col(\"computed_total_items\"))\n",
    "                 .otherwise(F.lit(None)))\n",
    ")\n",
    "\n",
    "\n",
    "display(df_items.limit(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.filter(F.col(\"computed_total_items\") > 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
